{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. Installation of publicly available programs used\n",
    "    1. Set up channels\n",
    "    2. Create an environment and install the packages\n",
    "2. Alignment\n",
    "    1. HISAT \n",
    "    2. Samtools\n",
    "    3. MultiQC\n",
    "3. ChIP-Seq\n",
    "    1. MACS\n",
    "    2. Bedtools\n",
    "4. RNA-Seq\n",
    "    1. HTSeq\n",
    "    2. featureCounts\n",
    "    3. DESeq\n",
    "    4. Salmon\n",
    "    5. dupRadar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation of publicly available programs used\n",
    "This jupyter notebook contains all of the code we need to run an RNA-Seq and ChIP-Seq pipeline, but it uses some publicly available software to take care of data acquisition, analysis, and visualization.\n",
    "\n",
    "To install this software, we are going to use a package manager that allows us to install software with a simple command.  We use Bioconda (https://bioconda.github.io/), which is a channel based on the conda package manager that specializes in bioinformatics packages. In conda, a channel contains a set of software managed by the same group.  Since we are using bioinformatics tools, we are using the Bioconda <i>channel</i> of the conda <i>package manager</i>.  There are over 3000 packages available through the Bioconda channel: https://bioconda.github.io/recipes.html#recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up channels\n",
    "You will need to add the bioconda channel as well as the other channels bioconda depends on. It is important to add them in the specific order below so that the priority is set correctly (that is, bioconda is highest priority).\n",
    "\n",
    "The conda-forge channel contains many general-purpose packages not already found in the defaults channel. The r channel is only included due to backward compatibility. It is not mandatory, but without the r channel, packages compiled against R 3.3.1 might not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This tutorial uses cells written in python and unix to perform its analyses. Lines that are written in unix are prefixed by an exclamation point. \n",
    "\n",
    "Select the following cell and run it. To run a cell, select the cell, click \"Cell\" the upper taskbar, and select \"Run Cells\". Or click the cell and press shift + enter.\n",
    "\n",
    "Alternatively, the contents of any cell may be copy+pasted into the terminal emulator to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda config --add channels defaults\n",
    "!conda config --add channels conda-forge\n",
    "!conda config --add channels bioconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an environment and install the packages\n",
    "\n",
    "In this tutorial we will create an environment named \"tutorial\" and install the packages in there. Environments offer ways of installing packages in specific environments so they can be managed and run for different specifications. You can create, export, list, remove and update environments that have different versions of Python and/or packages installed in them. Switching or moving between environments is called activating the environment. You can also share an environment file.\n",
    "\n",
    "This command will create an environment \"tutorial\" in which to install the packages used in this tutorial.\n",
    "\n",
    "Run the following commands to create the environment. When it asks if you want to proceed, type \"y\" and press enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n tutorial hisat2 multiqc macs2 salmon bioconductor-deseq matplotlib ggplot pybedtools samtools bioconductor-rsamtools bedtools htseq subread bioconductor-rsubread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then activate the environment with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Mac and Linux\n",
    "\n",
    "!source activate tutorial\n",
    "\n",
    "# For Windows\n",
    "\n",
    "!activate tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment\n",
    "## HISAT (Hierarchical Indexing for Spliced Alignment of Transcripts)\n",
    "\n",
    "In this tutorial, we'll use Hisat to align the sample reads to a reference genome. Hisat automatically downloads and preprocesses the reads so they're ready to be aligned. Hisat (hierarchical indexing for spliced alignment of transcripts) is a highly efficient system for aligning reads from RNA sequencing experiments. HISAT uses an indexing scheme based on the Burrows-Wheeler transform and the Ferragina-Manzini (FM) index, employing two types of indexes for alignment: a whole-genome FM index to anchor each alignment and numerous local FM indexes for very rapid extensions of these alignments. HISATâ€™s hierarchical index for the human genome contains 48,000 local FM indexes, each representing a genomic region of ~64,000 bp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNA-Seq data we'll use is from https://www.ncbi.nlm.nih.gov/Traces/study/?acc=SRP106028 and the ChIP-Seq data is from https://www.ncbi.nlm.nih.gov/Traces/study/?acc=SRP132584\n",
    "\n",
    "The model organism for this project is Yeast i.e. Saccharomyces cerevisiae. For RNA-Seq, yeast data between euploid and aneuoploid conditions will be compared. For ChIP-SEq, yeast data between 3AT-treated and untreated conditions will be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "RNASeqSRARunTableFile='data/RNASeqSRA.tsv'\n",
    "RNASeqSRATable = read_csv(RNASeqSRARunTableFile, delimiter='\\t')\n",
    "RNASeqoutrun = (RNASeqSRATable[\"Run\"]).astype(list)\n",
    "RNASeqoutputSam = \"test/\" + RNASeqoutrun + \".sam\"\n",
    "RNASeqoutputAlignmentSummary = \"test/\" + RNASeqoutrun + \".txt\"\n",
    "RNASeqoutputMetrics = \"test/\" + RNASeqoutrun + \".metrics\"\n",
    "RNASeqoutputSortBam = \"test/\" + RNASeqoutrun + \".sorted.bam\"\n",
    "\n",
    "ChIPSeqSRARunTableFile='data/ChIPSeqSRA.tsv'\n",
    "ChIPSeqSRATable = read_csv(ChIPSeqSRARunTableFile, delimiter='\\t')\n",
    "ChIPSeqoutrun = (ChIPSeqSRATable[\"Run\"]).astype(list)\n",
    "ChIPSeqoutputSam = \"test/\" + ChIPSeqoutrun + \".sam\"\n",
    "ChIPSeqoutputAlignmentSummary = \"test/\" + ChIPSeqoutrun + \".txt\"\n",
    "ChIPSeqoutputMetrics = \"test/\" + ChIPSeqoutrun + \".metrics\"\n",
    "ChIPSeqoutputSortBam = \"test/\" + ChIPSeqoutrun + \".sorted.bam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Then run the following command to create the yeast index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash scripts/make_yeast_index.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Align the RNA-Seq samples using Hisat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, individual in enumerate(RNASeqoutrun):\n",
    "    run = RNASeqoutrun[index]\n",
    "    summary = RNASeqoutputAlignmentSummary[index] \n",
    "    metrics = RNASeqoutputMetrics[index]\n",
    "    sam = RNASeqoutputSam[index]\n",
    "    bam = RNASeqoutputSortBam[index]\n",
    "    !hisat2 -x yeast_index/genome --sra-acc $run --new-summary --summary-file $summary --met-file $metrics -S $sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samtools \n",
    "\n",
    "We'll use samtools to sort the output files and convert them to bam files.\n",
    "\n",
    "Sort the output files and convert them to bam files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, individual in enumerate(RNASeqoutrun):\n",
    "    run = RNASeqoutrun[index]\n",
    "    summary = RNASeqoutputAlignmentSummary[index] \n",
    "    metrics = RNASeqoutputMetrics[index]\n",
    "    sam = RNASeqoutputSam[index]\n",
    "    bam = RNASeqoutputSortBam[index]\n",
    "    !samtools view -bSF4 $sam | samtools sort -o $bam\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Do the same thing for ChIP-Seq samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not open alignment output file ../../../test/SRR6703656.sam\n",
      "Error: Encountered internal HISAT2 exception (#1)\n",
      "Command: /home/ubuntu/miniconda3/envs/tutorial/bin/hisat2-align-s --wrapper basic-0 -x yeast_index/genome --sra-acc SRR6703656 --new-summary --summary-file ../.test/SRR6703656.txt --met-file ../../../test/SRR6703656.metrics -S ../../../test/SRR6703656.sam \n",
      "(ERR): hisat2-align exited with value 1\n",
      "Error: Could not open alignment output file ../../../test/SRR6703661.sam\n",
      "Error: Encountered internal HISAT2 exception (#1)\n",
      "Command: /home/ubuntu/miniconda3/envs/tutorial/bin/hisat2-align-s --wrapper basic-0 -x yeast_index/genome --sra-acc SRR6703661 --new-summary --summary-file ../.test/SRR6703661.txt --met-file ../../../test/SRR6703661.metrics -S ../../../test/SRR6703661.sam \n",
      "(ERR): hisat2-align exited with value 1\n",
      "Error: Could not open alignment output file ../../../test/SRR6703662.sam\n",
      "Error: Encountered internal HISAT2 exception (#1)\n",
      "Command: /home/ubuntu/miniconda3/envs/tutorial/bin/hisat2-align-s --wrapper basic-0 -x yeast_index/genome --sra-acc SRR6703662 --new-summary --summary-file ../.test/SRR6703662.txt --met-file ../../../test/SRR6703662.metrics -S ../../../test/SRR6703662.sam \n",
      "(ERR): hisat2-align exited with value 1\n",
      "Error: Could not open alignment output file ../../../test/SRR6703663.sam\n",
      "Error: Encountered internal HISAT2 exception (#1)\n",
      "Command: /home/ubuntu/miniconda3/envs/tutorial/bin/hisat2-align-s --wrapper basic-0 -x yeast_index/genome --sra-acc SRR6703663 --new-summary --summary-file ../.test/SRR6703663.txt --met-file ../../../test/SRR6703663.metrics -S ../../../test/SRR6703663.sam \n",
      "(ERR): hisat2-align exited with value 1\n"
     ]
    }
   ],
   "source": [
    "for index, individual in enumerate(ChIPSeqoutrun):\n",
    "    run = ChIPSeqoutrun[index]\n",
    "    summary = ChIPSeqoutputAlignmentSummary[index] \n",
    "    metrics = ChIPSeqoutputMetrics[index]\n",
    "    sam = ChIPSeqoutputSam[index]\n",
    "    bam = ChIPSeqoutputSortBam[index]\n",
    "    index = \"yeast_index/genome\"\n",
    "    !hisat2 -x $index --sra-acc $run --new-summary --summary-file $summary --met-file $metrics -S $sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, individual in enumerate(ChIPSeqoutrun):\n",
    "    run = ChIPSeqoutrun[index]\n",
    "    summary = ChIPSeqoutputAlignmentSummary[index] \n",
    "    metrics = ChIPSeqoutputMetrics[index]\n",
    "    sam = ChIPSeqoutputSam[index]\n",
    "    bam = ChIPSeqoutputSortBam[index]\n",
    "    !samtools view -bSF4 $sam | samtools sort -o $bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiQC\n",
    "\n",
    "This section details quality control checks on the read data from either RNAseq or ChIPseq data using MultiQC. MultiQC takes all output and log files from an alignment software program and aggregates the information from all samples into one convenient report (html by default).\n",
    "\n",
    "MultiQC was installed earlier in the tutorial, so all we need to do is run it on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiQC is configured to run the same no matter what type of sequencing data is available, therefore the same command can be used to analyze either our RNAseq data or our ChIPseq data.  We include the option 'hisat_output' since we are aligning using the HISAT2 program.  See http://multiqc.info/docs/ for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the 'hisat_output' option because we are analyzing data downloaded and aligned using the HISAT2 program.  We use the '--force' option to overwrite any previous versions of the multiqc_report.  '--quiet' only shows log warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!multiqc \"\".join(RNASeqoutrun) --quiet --outdir test/multiqc_rnaseq --force\n",
    "!multiqc \"\".join(ChIPSeqoutrun) --quiet --outdir test/multiqc_chipseq --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChIP-Seq\n",
    "## MACS (Model-based Analysis for ChIP-Seq)\n",
    "\n",
    "Peak-calling is one of the main steps scientists use in determining the locations where protein is bound in DNA. Peak detection software, such as MACS (Model-Based Analysis for ChIP-Seq), call peaks using the aligned sequecnes as input and returns precise locations of predicted peaks as output. In this tutorial, we'll use MACS.\n",
    "\n",
    "More information about MACS: http://liulab.dfci.harvard.edu/MACS/Download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Control sample SRR6703656\n",
      "2    Control sample SRR6703662\n",
      "Name: Run, dtype: object\n",
      "1    Treatment sample SRR6703661\n",
      "3    Treatment sample SRR6703663\n",
      "Name: Run, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ChIPSeqControl = ChIPSeqSRATable.loc[ChIPSeqSRATable[\"source_name\"] == \"Untreated\"][\"Run\"].astype(list)\n",
    "ChIPSeqTreatment = ChIPSeqSRATable.loc[ChIPSeqSRATable[\"source_name\"] != \"Untreated\"][\"Run\"].astype(list)\n",
    "\n",
    "print(\"Control sample \" + ChIPSeqControl)\n",
    "print(\"Treatment sample \" + ChIPSeqTreatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Wed, 18 Apr 2018 15:03:36: \n",
      "# Command line: callpeak -c test/SRR6703656.sorted.bam -t test/SRR6703661.sorted.bam -n test/SRR6703661.sorted.bam --outdir test/SRR6703661.sorted.bam\n",
      "# ARGUMENTS LIST:\n",
      "# name = test/SRR6703661.sorted.bam\n",
      "# format = AUTO\n",
      "# ChIP-seq file = ['test/SRR6703661.sorted.bam']\n",
      "# control file = ['test/SRR6703656.sorted.bam']\n",
      "# effective genome size = 2.70e+09\n",
      "# band width = 300\n",
      "# model fold = [5, 50]\n",
      "# qvalue cutoff = 5.00e-02\n",
      "# Larger dataset will be scaled towards smaller dataset.\n",
      "# Range for calculating regional lambda is: 1000 bps and 10000 bps\n",
      "# Broad region calling is off\n",
      "# Paired-End mode is off\n",
      " \n",
      "INFO  @ Wed, 18 Apr 2018 15:03:36: #1 read tag files... \n",
      "INFO  @ Wed, 18 Apr 2018 15:03:36: #1 read treatment tags... \n",
      "INFO  @ Wed, 18 Apr 2018 15:03:36: Detected format is: BAM \n",
      "INFO  @ Wed, 18 Apr 2018 15:03:36: * Input file is gzipped. \n",
      "INFO  @ Wed, 18 Apr 2018 15:03:40:  1000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:03:44:  2000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:03:48:  3000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:03:52:  4000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:03:56:  5000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:00:  6000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:04:  7000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:07:  8000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:11:  9000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:15:  10000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:19:  11000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:23:  12000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:27:  13000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:31:  14000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:35:  15000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:39:  16000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:43:  17000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:46:  18000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:50:  19000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:54:  20000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:04:58:  21000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:02:  22000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:06:  23000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:10:  24000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:14:  25000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:18:  26000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:22:  27000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:26:  28000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:29:  29000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:33:  30000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:37:  31000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:41:  32000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:45:  33000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:49:  34000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:53:  35000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:05:57:  36000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:01:  37000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:05:  38000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:09:  39000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:13:  40000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:16:  41000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:20:  42000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:24:  43000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:28:  44000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:32:  45000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:36:  46000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:40:  47000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:44:  48000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:48:  49000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:52:  50000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:55:  51000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:06:59:  52000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:03:  53000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:07:  54000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:10:  55000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:14:  56000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:18:  57000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:22:  58000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:26:  59000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:30:  60000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:34:  61000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:38:  62000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:42:  63000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:46:  64000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:49:  65000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:53:  66000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:07:57:  67000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:01:  68000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:05:  69000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:09:  70000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:13:  71000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:17:  72000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:20:  73000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:24:  74000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:28:  75000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:32:  76000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:36:  77000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:39:  78000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:43:  79000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:46:  80000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:50:  81000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:53:  82000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:08:56:  83000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:00:  84000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:03:  85000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:07:  86000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:10:  87000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:13:  88000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:17:  89000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:20:  90000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:24:  91000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:27:  92000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:30:  93000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:34:  94000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:37:  95000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:41:  96000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:44:  97000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:47:  98000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:51:  99000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:54:  100000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:09:58:  101000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:01:  102000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:04:  103000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:08:  104000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:11:  105000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:14:  106000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:18:  107000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:21:  108000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:24:  109000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:28:  110000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:31:  111000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:35:  112000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:39:  113000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:43:  114000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:47:  115000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:50:  116000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:54:  117000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:10:58:  118000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:02:  119000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:06:  120000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:10:  121000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:14:  122000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:18:  123000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:22:  124000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:26:  125000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:29:  126000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:33:  127000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:37:  128000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:41:  129000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:45:  130000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:49:  131000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:53:  132000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:11:57:  133000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:01:  134000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:05:  135000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:09:  136000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:13:  137000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:17:  138000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:21:  139000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:25:  140000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:29:  141000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:32:  142000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:36:  143000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:40:  144000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:44:  145000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:48:  146000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:52:  147000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:12:56:  148000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:00:  149000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:05: #1.2 read input tags... \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:05: Detected format is: BAM \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:05: * Input file is gzipped. \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:09:  1000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:12:  2000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:16:  3000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:20:  4000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:24:  5000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:28:  6000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Wed, 18 Apr 2018 15:13:32:  7000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:36:  8000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:39:  9000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:43:  10000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:47:  11000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:51:  12000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:55:  13000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:13:59:  14000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:03:  15000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:07:  16000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:11:  17000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:15:  18000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:19:  19000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:22:  20000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:26:  21000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:30:  22000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:34:  23000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:38:  24000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:42:  25000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:46:  26000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:50:  27000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:54:  28000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:14:58:  29000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:02:  30000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:06:  31000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:10:  32000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:13:  33000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:17:  34000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:21:  35000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:25:  36000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:29:  37000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:33:  38000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:37:  39000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:41:  40000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:45:  41000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:49:  42000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:53:  43000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:15:56:  44000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:00:  45000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:04:  46000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:08:  47000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:11:  48000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:15:  49000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:19:  50000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:23:  51000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:27:  52000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:31:  53000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:35:  54000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:39:  55000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:43:  56000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:47:  57000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:51:  58000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:54:  59000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:16:58:  60000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:02:  61000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:06:  62000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:10:  63000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:14:  64000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:18:  65000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:22:  66000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:25:  67000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:29:  68000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:33:  69000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:36:  70000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:40:  71000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:44:  72000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:48:  73000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:51:  74000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:55:  75000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:17:58:  76000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:01:  77000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:05:  78000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:08:  79000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:12:  80000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:15:  81000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:18:  82000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:22:  83000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:25:  84000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:29:  85000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:32:  86000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:35:  87000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:39:  88000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:42:  89000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:46:  90000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:49:  91000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:52:  92000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:56:  93000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:18:59:  94000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:03:  95000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:06:  96000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:09:  97000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:13:  98000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:16:  99000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:19:  100000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:23:  101000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:26:  102000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:29:  103000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:33:  104000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:36:  105000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:40:  106000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:43:  107000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:46:  108000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:49:  109000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:53:  110000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:19:57:  111000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:01:  112000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:05:  113000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:09:  114000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:13:  115000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:17:  116000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:20:  117000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:24:  118000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:28:  119000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:32:  120000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:36:  121000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:40:  122000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:44:  123000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:48:  124000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:52:  125000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:20:56:  126000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:00:  127000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:04:  128000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:08:  129000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:11:  130000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:15:  131000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:19:  132000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:23:  133000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:27:  134000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:31:  135000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:35:  136000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:39:  137000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:43:  138000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:47:  139000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:51:  140000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:55:  141000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:21:59:  142000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:03:  143000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:06:  144000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:10:  145000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:14:  146000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:18: #1 tag size is determined as 74 bps \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:18: #1 tag size = 74 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:18: #1  total tags in treatment: 58317085 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:18: #1 user defined the maximum tags... \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:18: #1 filter out redundant tags at the same location and the same strand by allowing at most 1 tag(s) \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:19: #1  tags after filtering in treatment: 4478491 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:19: #1  Redundant rate of treatment: 0.92 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:19: #1  total tags in control: 54125867 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:19: #1 user defined the maximum tags... \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:19: #1 filter out redundant tags at the same location and the same strand by allowing at most 1 tag(s) \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:20: #1  tags after filtering in control: 4162729 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:20: #1  Redundant rate of control: 0.92 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:20: #1 finished! \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:20: #2 Build Peak Model... \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:20: #2 looking for paired plus/minus strand peaks... \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:21: #2 number of paired peaks: 243 \n",
      "WARNING @ Wed, 18 Apr 2018 15:22:21: Fewer paired peaks (243) than 1000! Model may not be build well! Lower your MFOLD parameter may erase this warning. Now I will use 243 pairs to build model! \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:21: start model_add_line... \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:21: start X-correlation... \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:21: end of X-cor \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:21: #2 finished! \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:21: #2 predicted fragment length is 562 bps \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:21: #2 alternative fragment length(s) may be 473,503,525,562,589 bps \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:21: #2.2 Generate R script for model : test/SRR6703661.sorted.bam/test/SRR6703661.sorted.bam_model.r \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/envs/tutorial/bin/macs2\", line 4, in <module>\n",
      "    __import__('pkg_resources').run_script('MACS2==2.1.1.20160309', 'macs2')\n",
      "  File \"/home/ubuntu/miniconda3/envs/tutorial/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 658, in run_script\n",
      "    self.require(requires)[0].run_script(script_name, ns)\n",
      "  File \"/home/ubuntu/miniconda3/envs/tutorial/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 1445, in run_script\n",
      "    exec(script_code, namespace, namespace)\n",
      "  File \"/home/ubuntu/miniconda3/envs/tutorial/lib/python2.7/site-packages/MACS2-2.1.1.20160309-py2.7-linux-x86_64.egg/EGG-INFO/scripts/macs2\", line 617, in <module>\n",
      "    \n",
      "  File \"/home/ubuntu/miniconda3/envs/tutorial/lib/python2.7/site-packages/MACS2-2.1.1.20160309-py2.7-linux-x86_64.egg/EGG-INFO/scripts/macs2\", line 57, in main\n",
      "    \n",
      "  File \"build/bdist.linux-x86_64/egg/MACS2/callpeak_cmd.py\", line 188, in run\n",
      "  File \"build/bdist.linux-x86_64/egg/MACS2/OutputWriter.py\", line 243, in model2r_script\n",
      "IOError: [Errno 20] Not a directory: 'test/SRR6703661.sorted.bam/test/SRR6703661.sorted.bam_model.r'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Wed, 18 Apr 2018 15:22:23: \n",
      "# Command line: callpeak -c test/SRR6703662.sorted.bam -t test/SRR6703663.sorted.bam -n test/SRR6703663.sorted.bam --outdir test/SRR6703663.sorted.bam\n",
      "# ARGUMENTS LIST:\n",
      "# name = test/SRR6703663.sorted.bam\n",
      "# format = AUTO\n",
      "# ChIP-seq file = ['test/SRR6703663.sorted.bam']\n",
      "# control file = ['test/SRR6703662.sorted.bam']\n",
      "# effective genome size = 2.70e+09\n",
      "# band width = 300\n",
      "# model fold = [5, 50]\n",
      "# qvalue cutoff = 5.00e-02\n",
      "# Larger dataset will be scaled towards smaller dataset.\n",
      "# Range for calculating regional lambda is: 1000 bps and 10000 bps\n",
      "# Broad region calling is off\n",
      "# Paired-End mode is off\n",
      " \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:23: #1 read tag files... \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:23: #1 read treatment tags... \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:23: Detected format is: BAM \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:23: * Input file is gzipped. \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:27:  1000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:32:  2000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:36:  3000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:40:  4000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:44:  5000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:48:  6000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:52:  7000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:56:  8000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:22:59:  9000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:03:  10000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:06:  11000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:09:  12000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:12:  13000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:16:  14000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:19:  15000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:22:  16000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:26:  17000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:29:  18000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:32:  19000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:36:  20000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:39:  21000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:43:  22000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:46:  23000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:50:  24000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:53:  25000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:23:57:  26000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:01:  27000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:04:  28000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:08:  29000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:11:  30000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:15:  31000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:18:  32000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:22:  33000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:25:  34000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:29:  35000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:32:  36000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:36:  37000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:40:  38000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:43:  39000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:47:  40000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:50:  41000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:54:  42000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:24:57:  43000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:01:  44000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:04:  45000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:08:  46000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:12:  47000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:15:  48000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:19:  49000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:23:  50000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:26:  51000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:30:  52000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:33:  53000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:37:  54000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:40:  55000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:44:  56000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:47:  57000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:51:  58000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:54:  59000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:25:58:  60000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:01:  61000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:05:  62000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:09:  63000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:12:  64000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:16:  65000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:19:  66000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:22:  67000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:26:  68000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:30:  69000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:34:  70000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:37:  71000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:41:  72000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:44:  73000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:47:  74000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:51:  75000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:54:  76000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:26:58:  77000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:01:  78000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:05:  79000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:08:  80000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:12:  81000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:16:  82000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:19:  83000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:23:  84000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:26:  85000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:30:  86000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:33:  87000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:37:  88000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:40:  89000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:44:  90000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:47:  91000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:51:  92000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:54:  93000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:27:57:  94000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:01:  95000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:05:  96000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:08:  97000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:12:  98000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:16:  99000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:19:  100000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:23:  101000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:26:  102000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:29:  103000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:33:  104000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:36:  105000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:40:  106000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:43:  107000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:47:  108000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:50:  109000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:54:  110000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:28:57:  111000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:01:  112000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:05:  113000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:08:  114000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:12:  115000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:15:  116000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:19:  117000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:22:  118000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:26:  119000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:29:  120000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:33:  121000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:36:  122000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:39:  123000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:43:  124000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:47:  125000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:50:  126000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:54:  127000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:29:57:  128000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:01:  129000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:04:  130000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:07:  131000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:11:  132000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:14:  133000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:17:  134000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:21:  135000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:25:  136000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:28:  137000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:32:  138000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:35:  139000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:39:  140000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:43:  141000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:46:  142000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:50:  143000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:53:  144000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:30:57:  145000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:00:  146000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:04:  147000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:07:  148000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:10:  149000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:14:  150000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:17:  151000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:21:  152000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:24:  153000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:28:  154000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:32:  155000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:35:  156000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:39:  157000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:42:  158000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Wed, 18 Apr 2018 15:31:46:  159000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:49:  160000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:53:  161000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:31:56:  162000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:00:  163000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:03:  164000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:07:  165000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:10:  166000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:14:  167000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:17:  168000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:21:  169000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:24:  170000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:28:  171000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:31:  172000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:35:  173000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:38:  174000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:42:  175000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:45:  176000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:49:  177000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:52:  178000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:56:  179000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:32:59:  180000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:02:  181000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:06:  182000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:09:  183000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:13:  184000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:16:  185000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:20:  186000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:23:  187000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:27:  188000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:30:  189000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:34:  190000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:37:  191000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:41:  192000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:44:  193000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:48:  194000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:51:  195000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:55:  196000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:33:58:  197000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:02:  198000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:05:  199000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:09:  200000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:12:  201000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:15:  202000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:19:  203000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:22:  204000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:26:  205000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:29:  206000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:32:  207000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:36:  208000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:40:  209000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:43:  210000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:47:  211000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:50:  212000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:54:  213000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:34:57:  214000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:01:  215000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:04:  216000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:07:  217000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:11:  218000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:14:  219000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:18:  220000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:21:  221000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:24:  222000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:28:  223000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:31:  224000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:35:  225000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:38:  226000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:42:  227000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:45:  228000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:48:  229000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:52:  230000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:55:  231000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:35:58:  232000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:02:  233000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:06:  234000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:10:  235000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:15:  236000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:16: #1.2 read input tags... \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:16: Detected format is: BAM \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:16: * Input file is gzipped. \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:21:  1000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:25:  2000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:29:  3000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:33:  4000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:36:  5000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:39:  6000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:42:  7000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:46:  8000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:49:  9000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:52:  10000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:56:  11000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:36:59:  12000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:02:  13000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:06:  14000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:09:  15000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:13:  16000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:16:  17000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:20:  18000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:23:  19000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:27:  20000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:31:  21000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:34:  22000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:38:  23000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:41:  24000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:45:  25000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:48:  26000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:52:  27000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:55:  28000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:37:59:  29000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:02:  30000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:06:  31000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:09:  32000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:13:  33000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:16:  34000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:20:  35000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:24:  36000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:28:  37000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:31:  38000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:35:  39000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:39:  40000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:43:  41000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:46:  42000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:50:  43000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:54:  44000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:38:57:  45000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:01:  46000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:05:  47000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:09:  48000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:12:  49000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:16:  50000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:20:  51000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:24:  52000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:28:  53000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:31:  54000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:35:  55000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:38:  56000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:43:  57000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:47:  58000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:51:  59000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:54:  60000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:39:58:  61000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:01:  62000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:05:  63000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:08:  64000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:12:  65000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:15:  66000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:19:  67000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:23:  68000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:27:  69000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:31:  70000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:35:  71000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:39:  72000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:42:  73000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:45:  74000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:49:  75000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:52:  76000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:40:56:  77000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:00:  78000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:03:  79000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:07:  80000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:10:  81000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:15:  82000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:19:  83000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:23:  84000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:27:  85000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:31:  86000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:34:  87000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:37:  88000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:41:  89000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:44:  90000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:48:  91000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:52:  92000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:41:56:  93000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:00:  94000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:03:  95000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Wed, 18 Apr 2018 15:42:07:  96000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:11:  97000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:15:  98000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:19:  99000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:23:  100000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:26:  101000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:30:  102000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:34:  103000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:37:  104000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:41:  105000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:44:  106000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:48:  107000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:51:  108000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:55:  109000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:42:59:  110000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:03:  111000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:08:  112000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:11:  113000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:14:  114000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:18:  115000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:21:  116000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:25:  117000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:28:  118000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:32:  119000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:35:  120000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:39:  121000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:43:  122000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:47:  123000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:51:  124000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:54:  125000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:43:58:  126000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:01:  127000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:05:  128000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:09:  129000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:12:  130000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:16:  131000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:19:  132000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:23:  133000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:26:  134000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:30:  135000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:33:  136000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:37:  137000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:41:  138000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:44:  139000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:48:  140000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:52:  141000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:55:  142000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:44:59:  143000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:03:  144000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:06:  145000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:10:  146000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:13:  147000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:17:  148000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:20:  149000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:24:  150000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:27:  151000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:31:  152000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:34:  153000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:38:  154000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:41:  155000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:45:  156000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:48:  157000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:52:  158000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:55:  159000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:45:59:  160000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:02:  161000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:06:  162000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:09:  163000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:13:  164000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:16:  165000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:20:  166000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:24:  167000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:28:  168000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:31:  169000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:34:  170000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:38:  171000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:41:  172000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:45:  173000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:48:  174000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:52:  175000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:55:  176000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:46:59:  177000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:02:  178000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:06:  179000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:09:  180000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:13:  181000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:16:  182000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:19:  183000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:23:  184000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:26:  185000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:30:  186000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:33:  187000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:37:  188000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:40:  189000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:44:  190000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:48:  191000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:51:  192000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:55:  193000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:47:58:  194000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:02:  195000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:05:  196000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:09:  197000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:12:  198000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:15:  199000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:19:  200000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:22:  201000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:26:  202000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:29:  203000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:32:  204000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:36:  205000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:39:  206000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:43:  207000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:46:  208000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:49:  209000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:53:  210000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:48:57:  211000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:01:  212000000 \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:03: #1 tag size is determined as 75 bps \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:03: #1 tag size = 75 \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:03: #1  total tags in treatment: 44717956 \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:03: #1 user defined the maximum tags... \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:03: #1 filter out redundant tags at the same location and the same strand by allowing at most 1 tag(s) \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:04: #1  tags after filtering in treatment: 1656824 \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:04: #1  Redundant rate of treatment: 0.96 \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:04: #1  total tags in control: 39235343 \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:04: #1 user defined the maximum tags... \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:04: #1 filter out redundant tags at the same location and the same strand by allowing at most 1 tag(s) \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:05: #1  tags after filtering in control: 926065 \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:05: #1  Redundant rate of control: 0.98 \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:05: #1 finished! \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:05: #2 Build Peak Model... \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:05: #2 looking for paired plus/minus strand peaks... \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:05: #2 number of paired peaks: 187 \n",
      "WARNING @ Wed, 18 Apr 2018 15:49:05: Fewer paired peaks (187) than 1000! Model may not be build well! Lower your MFOLD parameter may erase this warning. Now I will use 187 pairs to build model! \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:05: start model_add_line... \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:05: start X-correlation... \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:05: end of X-cor \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:05: #2 finished! \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:05: #2 predicted fragment length is 28 bps \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:05: #2 alternative fragment length(s) may be 1,28,46,67,91,112,136,160,212,248,319,342,425,445,520,557,594 bps \n",
      "INFO  @ Wed, 18 Apr 2018 15:49:05: #2.2 Generate R script for model : test/SRR6703663.sorted.bam/test/SRR6703663.sorted.bam_model.r \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/envs/tutorial/bin/macs2\", line 4, in <module>\n",
      "    __import__('pkg_resources').run_script('MACS2==2.1.1.20160309', 'macs2')\n",
      "  File \"/home/ubuntu/miniconda3/envs/tutorial/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 658, in run_script\n",
      "    self.require(requires)[0].run_script(script_name, ns)\n",
      "  File \"/home/ubuntu/miniconda3/envs/tutorial/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 1445, in run_script\n",
      "    exec(script_code, namespace, namespace)\n",
      "  File \"/home/ubuntu/miniconda3/envs/tutorial/lib/python2.7/site-packages/MACS2-2.1.1.20160309-py2.7-linux-x86_64.egg/EGG-INFO/scripts/macs2\", line 617, in <module>\n",
      "    \n",
      "  File \"/home/ubuntu/miniconda3/envs/tutorial/lib/python2.7/site-packages/MACS2-2.1.1.20160309-py2.7-linux-x86_64.egg/EGG-INFO/scripts/macs2\", line 57, in main\n",
      "    \n",
      "  File \"build/bdist.linux-x86_64/egg/MACS2/callpeak_cmd.py\", line 188, in run\n",
      "  File \"build/bdist.linux-x86_64/egg/MACS2/OutputWriter.py\", line 243, in model2r_script\n",
      "IOError: [Errno 20] Not a directory: 'test/SRR6703663.sorted.bam/test/SRR6703663.sorted.bam_model.r'\n"
     ]
    }
   ],
   "source": [
    "for index, individual in enumerate(ChIPSeqControl):\n",
    "    outputdirectory = \"test/\" + ChIPSeqTreatment.iloc[index]\n",
    "    immunoprecipitate = \"test/\" + ChIPSeqTreatment.iloc[index] + \".sorted.bam\"\n",
    "    sample = \"test/\" + ChIPSeqControl.iloc[index] + \".sorted.bam\"\n",
    "    !macs2 callpeak -c $sample -t $immunoprecipitate -n $immunoprecipitate --outdir outputdirectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an in-depth discussion of what MACS2 does: https://github.com/taoliu/MACS/wiki/Advanced:-Call-peaks-using-MACS2-subcommands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedtools\n",
    "\n",
    "In this tutorial, we'll use Bedtools to extract the intersecting regions of the MACS output between the experimental conditions.\n",
    "\n",
    "The Bedtools suite of programs is widely used for genomic interval manipulation or \"genome algebra\". pybedtools wraps and extends Bedtools and offers feature-level manipulations from within Python.\n",
    "\n",
    "First we'll sort the output. The following line uses the `sort` command to sort the MACS output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sort -k 1,1 -k2,2n test/SRR6703661/SRR6703661.narrowPeak > test/SRR6703661/SRR6703661.narrowPeak.sorted\n",
    "!sort -k 1,1 -k2,2n test/SRR6703663/SRR6703663.narrowPeak > test/SRR6703661/SRR6703663.narrowPeak.sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll use pybedtools to truncate the output files to chromosome so no ends are out of bounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybedtools import BedTool\n",
    "\n",
    "BedTool(test/SRR6703661/SRR6703661.narrowPeak.sorted).truncate_to_chrom('sacCer3').saveas(test/SRR6703661/SRR6703661.narrowPeak.sorted.truncated)\n",
    "BedTool(test/SRR6703661/SRR6703663.narrowPeak.sorted).truncate_to_chrom('sacCer3').saveas(test/SRR6703661/SRR6703663.narrowPeak.sorted.truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll find the intersecting regions between the experimental conditions and the control conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bedtools intersect -a test/SRR6703661/SRR6703661.narrowPeak.sorted.truncated -b test/SRR6703661/SRR6703663.narrowPeak.sorted.truncated -u > test/ChIPSeqintersect.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNA-Seq\n",
    "## HTSeq (High-through sequencing)\n",
    "\n",
    "HTSeq is a Python library to facilitate the rapid development of RNA-Seq analysis. HTSeq offers parsers for many common data formats in HTS projects, as well as classes to represent data, such as genomic coordinates, sequences, sequencing reads, alignments, gene model information and variant calls, and provides data structures that allow for querying via genomic coordinates. In this tutorial we will use htseq-count, a tool developed with HTSeq that preprocesses RNA-Seq data for differential expression analysis by counting the overlap of reads with genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "gtf = \"data/Saccharomyces_cerevisiae.R64-1-1.84.gtf\"\n",
    "\n",
    "RNASeqSRARunTableFile='data/RNASeqSRA.tsv'\n",
    "RNASeqSRATable = read_csv(RNASeqSRARunTableFile, delimiter='\\t')\n",
    "RNASeqoutrun = (RNASeqSRATable[\"Run\"]).astype(list)\n",
    "RNASeqoutputSortBam = \"test/\" + RNASeqoutrun + \".sorted.bam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, individual in enumerate(RNASeqoutputSortBam):\n",
    "    input = individual\n",
    "    output = individual + \".genecount.txt\"\n",
    "    !htseq-count -m intersection-nonempty -s no -f bam $input $gtf > $output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## featureCounts\n",
    "\n",
    "featureCounts is a highly efficient general-purpose read summarization program that counts mapped reads for genomic features such as genes, exons, promoter, gene bodies, genomic bins and chromosomal locations. It can be used to count both RNA-seq and genomic DNA-seq reads. It is available in the SourceForge Subread package or the Bioconductor Rsubread package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, individual in enumerate(RNASeqoutputSortBam):\n",
    "    input = individual\n",
    "    !featureCounts -s -a $gtf -o test/featureCounts $input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESeq (Differential Expression Sequencing)\n",
    "\n",
    "Estimate variance-mean dependence in count data from high-throughput sequencing assays and test for differential expression based on a model using the negative binomial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!Rscript scripts/runDeseq.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salmon\n",
    "\n",
    "Salmon is a tool for quantifying the expression of transcripts using RNA-seq data. Salmon uses new algorithms (specifically, coupling the concept of quasi-mapping with a two-phase inference procedure) to provide accurate expression estimates very quickly (i.e. wicked-fast) and while using little memory. Salmon performs its inference using an expressive and realistic model of RNA-seq data that takes into account experimental attributes and biases commonly observed in real RNA-seq data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O data/transcript.fa.gz ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/146/045/GCF_000146045.2_R64/GCF_000146045.2_R64_rna.fna.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Then unzip the transcript.fa.gz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip data/transcript.fa.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finally, create the salmon index and run salmon on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salmon_index = \"test/yeast-salmon.idx\"\n",
    "\n",
    "!salmon index -t data/transcript.fa -i $salmon_index --type quasi -k 31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "RNASeqSRARunTableFile='data/RNASeqSRA.tsv'\n",
    "RNASeqSRATable = read_csv(RNASeqSRARunTableFile, delimiter='\\t')\n",
    "RNASeqoutrun = (RNASeqSRATable[\"Run\"])\n",
    "RNASeqBam = \"test/\" + RNASeqoutrun + \".sorted.bam\"\n",
    "\n",
    "for index, individual in enumerate(RNASeqBam):\n",
    "    !salmon quant --targets $salmon_index -l A --output test -a $individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dupRadar\n",
    "dupRadar provides an easy way to distinguish between artifactual vs natural duplicate reads in RNA-Seq data. Prior to dupRadar only global duplication rates were used and they don't take into account the effect of gene expression levels. dupRadar relates duplication rates and length normalized read counts of every gene to model the dependency of both variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, individual in enumerate(RNASeqoutputSortBam):\n",
    "    !Rscript scripts/dupRadar.R $individual $gtf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
