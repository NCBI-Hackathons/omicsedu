{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeqAcademy Multiomics Tutorial\n",
    "\n",
    "# Contents\n",
    "1. Installation\n",
    "    1. Set up channels\n",
    "    2. Create an environment and install the packages\n",
    "2. Alignment\n",
    "    1. HISAT \n",
    "    2. Samtools\n",
    "    3. MultiQC\n",
    "3. ChIP-Seq\n",
    "    1. MACS\n",
    "    2. Bedtools\n",
    "4. RNA-Seq\n",
    "    1. HTSeq\n",
    "    2. featureCounts\n",
    "    3. DESeq\n",
    "    4. Salmon\n",
    "    5.dupRadar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "Before running any programs, we'll make sure that each software is installed correctly. This tutorial uses Bioconda (https://bioconda.github.io/). Bioconda is a channel for the conda package manager specializing in bioinformatics software. The available packages are listed here: https://bioconda.github.io/recipes.html#recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up channels\n",
    "\n",
    "You will need to add the bioconda channel as well as the other channels bioconda depends on. It is important to add them in this order so that the priority is set correctly (that is, bioconda is highest priority).\n",
    "\n",
    "The conda-forge channel contains many general-purpose packages not already found in the defaults channel. The r channel is only included due to backward compatibility. It is not mandatory, but without the r channel packages compiled against R 3.3.1 might not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses cells written in python and unix to perform its analyses. Lines that are written in unix are prefixed by an exclamation point. \n",
    "\n",
    "Select the following cell and run it. To run a cell, select the cell, click \"Cell\" the upper taskbar, and select \"Run Cells\". Or click the cell and press shift + enter.\n",
    "\n",
    "Alternatively, the contents of any cell may be copy+pasted into the terminal emulator to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda config --add channels defaults\n",
    "!conda config --add channels conda-forge\n",
    "!conda config --add channels bioconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an environment and install the packages\n",
    "\n",
    "In this tutorial we will create an environment named \"tutorial\" and install the packages in there. Environments offer ways of installing packages in specific environments so they can be managed and run for different specifications. You can create, export, list, remove and update environments that have different versions of Python and/or packages installed in them. Switching or moving between environments is called activating the environment. You can also share an environment file.\n",
    "\n",
    "This command will create an environment \"tutorial\" in which to install the packages used in this tutorial.\n",
    "\n",
    "Run the following commands to create the environment. When it asks if you want to proceed, type \"y\" and press enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n tutorial hisat2 multiqc macs2 salmon bioconductor-deseq matplotlib ggplot pybedtools samtools bioconductor-rsamtools bedtools htseq subread bioconductor-rsubread deeptools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then activate the environment with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Mac and Linux\n",
    "\n",
    "!source activate tutorial\n",
    "\n",
    "# For Windows\n",
    "\n",
    "!activate tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment\n",
    "## HISAT (Hierarchical Indexing for Spliced Alignment of Transcripts)\n",
    "\n",
    "In this tutorial, we'll use Hisat to align the sample reads to a reference genome. Hisat automatically downloads and preprocesses the reads so they're ready to be aligned. Hisat (hierarchical indexing for spliced alignment of transcripts) is a highly efficient system for aligning reads from RNA sequencing experiments. HISAT uses an indexing scheme based on the Burrows-Wheeler transform and the Ferragina-Manzini (FM) index, employing two types of indexes for alignment: a whole-genome FM index to anchor each alignment and numerous local FM indexes for very rapid extensions of these alignments. HISATâ€™s hierarchical index for the human genome contains 48,000 local FM indexes, each representing a genomic region of ~64,000 bp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNA-Seq data we'll use is from https://www.ncbi.nlm.nih.gov/Traces/study/?acc=SRP106028 and the ChIP-Seq data is from https://www.ncbi.nlm.nih.gov/Traces/study/?acc=SRP132584\n",
    "\n",
    "The model organism for this project is Yeast i.e. Saccharomyces cerevisiae. For RNA-Seq, yeast data between euploid and aneuoploid conditions will be compared. For ChIP-SEq, yeast data between 3AT-treated and untreated conditions will be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "RNASeqSRARunTableFile='data/RNASeqSRA.tsv'\n",
    "RNASeqSRATable = read_csv(RNASeqSRARunTableFile, delimiter='\\t')\n",
    "RNASeqoutrun = (RNASeqSRATable[\"Run\"]).astype(list)\n",
    "RNASeqoutputSam = \"test/\" + RNASeqoutrun + \".sam\"\n",
    "RNASeqoutputAlignmentSummary = \"test/\" + RNASeqoutrun + \".txt\"\n",
    "RNASeqoutputMetrics = \"test/\" + RNASeqoutrun + \".metrics\"\n",
    "RNASeqoutputSortBam = \"test/\" + RNASeqoutrun + \".sorted.bam\"\n",
    "\n",
    "ChIPSeqSRARunTableFile='data/ChIPSeqSRA.tsv'\n",
    "ChIPSeqSRATable = read_csv(ChIPSeqSRARunTableFile, delimiter='\\t')\n",
    "ChIPSeqoutrun = (ChIPSeqSRATable[\"Run\"]).astype(list)\n",
    "ChIPSeqoutputSam = \"test/\" + ChIPSeqoutrun + \".sam\"\n",
    "ChIPSeqoutputAlignmentSummary = \"test/\" + ChIPSeqoutrun + \".txt\"\n",
    "ChIPSeqoutputMetrics = \"test/\" + ChIPSeqoutrun + \".metrics\"\n",
    "ChIPSeqoutputSortBam = \"test/\" + ChIPSeqoutrun + \".sorted.bam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Then run the following command to create the yeast index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash scripts/make_yeast_index.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Align the RNA-Seq samples using Hisat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, individual in enumerate(RNASeqoutrun):\n",
    "    run = RNASeqoutrun[index]\n",
    "    summary = RNASeqoutputAlignmentSummary[index] \n",
    "    metrics = RNASeqoutputMetrics[index]\n",
    "    sam = RNASeqoutputSam[index]\n",
    "    bam = RNASeqoutputSortBam[index]\n",
    "    !hisat2 -x yeast_index/genome --sra-acc $run --new-summary --summary-file $summary --met-file $metrics -S $sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samtools \n",
    "\n",
    "We'll use samtools to sort the output files and convert them to bam files.\n",
    "\n",
    "Sort the output files and convert them to bam files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, individual in enumerate(RNASeqoutrun):\n",
    "    run = RNASeqoutrun[index]\n",
    "    summary = RNASeqoutputAlignmentSummary[index] \n",
    "    metrics = RNASeqoutputMetrics[index]\n",
    "    sam = RNASeqoutputSam[index]\n",
    "    bam = RNASeqoutputSortBam[index]\n",
    "    !samtools view -bSF4 $sam | samtools sort -o $bam\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Do the same thing for ChIP-Seq samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, individual in enumerate(ChIPSeqoutrun):\n",
    "    run = ChIPSeqoutrun[index]\n",
    "    summary = ChIPSeqoutputAlignmentSummary[index] \n",
    "    metrics = ChIPSeqoutputMetrics[index]\n",
    "    sam = ChIPSeqoutputSam[index]\n",
    "    bam = ChIPSeqoutputSortBam[index]\n",
    "    index = \"yeast_index/genome\"\n",
    "    !hisat2 -x $index --sra-acc $run --new-summary --summary-file $summary --met-file $metrics -S $sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, individual in enumerate(ChIPSeqoutrun):\n",
    "    run = ChIPSeqoutrun[index]\n",
    "    summary = ChIPSeqoutputAlignmentSummary[index] \n",
    "    metrics = ChIPSeqoutputMetrics[index]\n",
    "    sam = ChIPSeqoutputSam[index]\n",
    "    bam = ChIPSeqoutputSortBam[index]\n",
    "    !samtools view -bSF4 $sam | samtools sort -o $bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiQC\n",
    "\n",
    "This section details quality control checks on the read data from either RNAseq or ChIPseq data using MultiQC. MultiQC takes all output and log files from an alignment software program and aggregates the information from all samples into one convenient report (html by default).\n",
    "\n",
    "MultiQC was installed earlier in the tutorial, so all we need to do is run it on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiQC is configured to run the same no matter what type of sequencing data is available, therefore the same command can be used to analyze either our RNAseq data or our ChIPseq data.  We include the option 'hisat_output' since we are aligning using the HISAT2 program.  See http://multiqc.info/docs/ for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the 'hisat_output' option because we are analyzing data downloaded and aligned using the HISAT2 program.  We use the '--force' option to overwrite any previous versions of the multiqc_report.  '--quiet' only shows log warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!multiqc \"\".join(RNASeqoutrun) --quiet --outdir test/multiqc_rnaseq --force\n",
    "!multiqc \"\".join(ChIPSeqoutrun) --quiet --outdir test/multiqc_chipseq --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChIP-Seq\n",
    "## MACS (Model-based Analysis for ChIP-Seq)\n",
    "\n",
    "Peak-calling is one of the main steps scientists use in determining the locations where protein is bound in DNA. Peak detection software, such as MACS (Model-Based Analysis for ChIP-Seq), call peaks using the aligned sequecnes as input and returns precise locations of predicted peaks as output. In this tutorial, we'll use MACS.\n",
    "\n",
    "More information about MACS: http://liulab.dfci.harvard.edu/MACS/Download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Control sample SRR6703656\n",
      "2    Control sample SRR6703662\n",
      "Name: Run, dtype: object\n",
      "1    Treatment sample SRR6703661\n",
      "3    Treatment sample SRR6703663\n",
      "Name: Run, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ChIPSeqControl = ChIPSeqSRATable.loc[ChIPSeqSRATable[\"source_name\"] == \"Untreated\"][\"Run\"].astype(list)\n",
    "ChIPSeqTreatment = ChIPSeqSRATable.loc[ChIPSeqSRATable[\"source_name\"] != \"Untreated\"][\"Run\"].astype(list)\n",
    "\n",
    "print(\"Control sample \" + ChIPSeqControl)\n",
    "print(\"Treatment sample \" + ChIPSeqTreatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Wed, 18 Apr 2018 16:11:33: \n",
      "# Command line: callpeak -c test/SRR6703656.sorted.bam -t test/SRR6703661.sorted.bam -n SRR6703661 --outdir test/SRR6703661\n",
      "# ARGUMENTS LIST:\n",
      "# name = SRR6703661\n",
      "# format = AUTO\n",
      "# ChIP-seq file = ['test/SRR6703661.sorted.bam']\n",
      "# control file = ['test/SRR6703656.sorted.bam']\n",
      "# effective genome size = 2.70e+09\n",
      "# band width = 300\n",
      "# model fold = [5, 50]\n",
      "# qvalue cutoff = 5.00e-02\n",
      "# Larger dataset will be scaled towards smaller dataset.\n",
      "# Range for calculating regional lambda is: 1000 bps and 10000 bps\n",
      "# Broad region calling is off\n",
      "# Paired-End mode is off\n",
      " \n",
      "INFO  @ Wed, 18 Apr 2018 16:11:33: #1 read tag files... \n",
      "INFO  @ Wed, 18 Apr 2018 16:11:33: #1 read treatment tags... \n",
      "INFO  @ Wed, 18 Apr 2018 16:11:33: Detected format is: BAM \n",
      "INFO  @ Wed, 18 Apr 2018 16:11:33: * Input file is gzipped. \n",
      "INFO  @ Wed, 18 Apr 2018 16:11:37:  1000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:11:41:  2000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:11:45:  3000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:11:49:  4000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:11:53:  5000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:11:57:  6000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:00:  7000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:04:  8000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:08:  9000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:12:  10000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:16:  11000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:20:  12000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:24:  13000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:28:  14000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:31:  15000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:35:  16000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:39:  17000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:43:  18000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:47:  19000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:51:  20000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:55:  21000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:12:59:  22000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:03:  23000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:07:  24000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:11:  25000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:14:  26000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:18:  27000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:22:  28000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:26:  29000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:30:  30000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:34:  31000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:38:  32000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:42:  33000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:46:  34000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:50:  35000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:54:  36000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:13:58:  37000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:02:  38000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:05:  39000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:09:  40000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:13:  41000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:17:  42000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:21:  43000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:25:  44000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:29:  45000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:33:  46000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:37:  47000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:41:  48000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:45:  49000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:49:  50000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:52:  51000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:14:56:  52000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:00:  53000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:04:  54000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:07:  55000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:11:  56000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:15:  57000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:19:  58000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:23:  59000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:27:  60000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:31:  61000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:35:  62000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:39:  63000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:43:  64000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:47:  65000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:50:  66000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:54:  67000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:15:58:  68000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:02:  69000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:06:  70000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:10:  71000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:14:  72000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:18:  73000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:21:  74000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:25:  75000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:29:  76000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:33:  77000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:37:  78000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:40:  79000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:44:  80000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:47:  81000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:50:  82000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:54:  83000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:16:57:  84000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:01:  85000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:04:  86000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:07:  87000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:11:  88000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:14:  89000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:17:  90000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:21:  91000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:24:  92000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:28:  93000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:31:  94000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:35:  95000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:38:  96000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:41:  97000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:45:  98000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:48:  99000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:51:  100000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:55:  101000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:17:58:  102000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:01:  103000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:05:  104000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:08:  105000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:12:  106000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:15:  107000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:18:  108000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:22:  109000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:25:  110000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:28:  111000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:32:  112000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:36:  113000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:40:  114000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:44:  115000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:48:  116000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:52:  117000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:55:  118000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:18:59:  119000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:03:  120000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:07:  121000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:11:  122000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:15:  123000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:19:  124000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:23:  125000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:27:  126000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:31:  127000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:35:  128000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:39:  129000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:43:  130000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:47:  131000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:51:  132000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:54:  133000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:19:58:  134000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:02:  135000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:06:  136000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:10:  137000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:14:  138000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:18:  139000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:22:  140000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:26:  141000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:30:  142000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:34:  143000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:38:  144000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:42:  145000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:46:  146000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:50:  147000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:54:  148000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:20:58:  149000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:02: #1.2 read input tags... \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:02: Detected format is: BAM \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:02: * Input file is gzipped. \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:06:  1000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:10:  2000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:14:  3000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:18:  4000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:22:  5000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:26:  6000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:30:  7000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Wed, 18 Apr 2018 16:21:34:  8000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:37:  9000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:41:  10000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:45:  11000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:49:  12000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:53:  13000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:21:57:  14000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:01:  15000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:05:  16000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:09:  17000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:13:  18000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:17:  19000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:21:  20000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:25:  21000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:29:  22000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:32:  23000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:36:  24000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:40:  25000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:44:  26000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:48:  27000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:52:  28000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:22:56:  29000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:00:  30000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:04:  31000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:08:  32000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:12:  33000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:16:  34000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:20:  35000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:24:  36000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:28:  37000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:32:  38000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:36:  39000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:40:  40000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:44:  41000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:47:  42000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:51:  43000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:55:  44000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:23:59:  45000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:03:  46000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:07:  47000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:10:  48000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:14:  49000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:18:  50000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:22:  51000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:26:  52000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:30:  53000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:34:  54000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:38:  55000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:42:  56000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:46:  57000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:50:  58000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:53:  59000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:24:58:  60000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:02:  61000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:05:  62000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:09:  63000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:13:  64000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:17:  65000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:21:  66000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:25:  67000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:29:  68000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:32:  69000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:36:  70000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:40:  71000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:44:  72000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:47:  73000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:51:  74000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:54:  75000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:25:57:  76000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:01:  77000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:04:  78000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:08:  79000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:11:  80000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:14:  81000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:18:  82000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:21:  83000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:25:  84000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:28:  85000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:31:  86000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:35:  87000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:38:  88000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:42:  89000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:45:  90000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:48:  91000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:52:  92000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:55:  93000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:26:59:  94000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:02:  95000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:05:  96000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:09:  97000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:12:  98000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:15:  99000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:19:  100000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:22:  101000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:25:  102000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:29:  103000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:32:  104000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:36:  105000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:39:  106000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:42:  107000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:46:  108000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:49:  109000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:52:  110000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:27:56:  111000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:00:  112000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:04:  113000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:08:  114000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:12:  115000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:16:  116000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:20:  117000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:24:  118000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:28:  119000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:32:  120000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:36:  121000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:40:  122000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:43:  123000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:47:  124000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:51:  125000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:55:  126000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:28:59:  127000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:03:  128000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:07:  129000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:11:  130000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:15:  131000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:19:  132000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:23:  133000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:27:  134000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:31:  135000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:35:  136000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:38:  137000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:42:  138000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:46:  139000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:50:  140000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:54:  141000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:29:58:  142000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:02:  143000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:06:  144000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:10:  145000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:14:  146000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:18: #1 tag size is determined as 74 bps \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:18: #1 tag size = 74 \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:18: #1  total tags in treatment: 58317085 \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:18: #1 user defined the maximum tags... \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:18: #1 filter out redundant tags at the same location and the same strand by allowing at most 1 tag(s) \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:19: #1  tags after filtering in treatment: 4478491 \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:19: #1  Redundant rate of treatment: 0.92 \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:19: #1  total tags in control: 54125867 \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:19: #1 user defined the maximum tags... \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:19: #1 filter out redundant tags at the same location and the same strand by allowing at most 1 tag(s) \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:20: #1  tags after filtering in control: 4162729 \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:20: #1  Redundant rate of control: 0.92 \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:20: #1 finished! \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:20: #2 Build Peak Model... \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:20: #2 looking for paired plus/minus strand peaks... \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:20: #2 number of paired peaks: 243 \n",
      "WARNING @ Wed, 18 Apr 2018 16:30:20: Fewer paired peaks (243) than 1000! Model may not be build well! Lower your MFOLD parameter may erase this warning. Now I will use 243 pairs to build model! \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:20: start model_add_line... \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:20: start X-correlation... \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:20: end of X-cor \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:20: #2 finished! \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:20: #2 predicted fragment length is 562 bps \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:20: #2 alternative fragment length(s) may be 473,503,525,562,589 bps \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:20: #2.2 Generate R script for model : test/SRR6703661/SRR6703661_model.r \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Wed, 18 Apr 2018 16:30:20: #3 Call peaks... \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:20: #3 Pre-compute pvalue-qvalue table... \n",
      "INFO  @ Wed, 18 Apr 2018 16:30:58: #3 Call peaks for each chromosome... \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:03: #4 Write output xls file... test/SRR6703661/SRR6703661_peaks.xls \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:03: #4 Write peak in narrowPeak format file... test/SRR6703661/SRR6703661_peaks.narrowPeak \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:03: #4 Write summits bed file... test/SRR6703661/SRR6703661_summits.bed \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:03: Done! \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:06: \n",
      "# Command line: callpeak -c test/SRR6703662.sorted.bam -t test/SRR6703663.sorted.bam -n SRR6703663 --outdir test/SRR6703663\n",
      "# ARGUMENTS LIST:\n",
      "# name = SRR6703663\n",
      "# format = AUTO\n",
      "# ChIP-seq file = ['test/SRR6703663.sorted.bam']\n",
      "# control file = ['test/SRR6703662.sorted.bam']\n",
      "# effective genome size = 2.70e+09\n",
      "# band width = 300\n",
      "# model fold = [5, 50]\n",
      "# qvalue cutoff = 5.00e-02\n",
      "# Larger dataset will be scaled towards smaller dataset.\n",
      "# Range for calculating regional lambda is: 1000 bps and 10000 bps\n",
      "# Broad region calling is off\n",
      "# Paired-End mode is off\n",
      " \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:06: #1 read tag files... \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:06: #1 read treatment tags... \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:06: Detected format is: BAM \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:06: * Input file is gzipped. \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:10:  1000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:14:  2000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:18:  3000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:22:  4000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:26:  5000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:31:  6000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:35:  7000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:38:  8000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:42:  9000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:45:  10000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:48:  11000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:51:  12000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:54:  13000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:31:58:  14000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:01:  15000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:04:  16000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:07:  17000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:11:  18000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:14:  19000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:18:  20000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:21:  21000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:25:  22000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:28:  23000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:32:  24000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:35:  25000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:39:  26000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:42:  27000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:46:  28000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:49:  29000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:53:  30000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:32:56:  31000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:00:  32000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:03:  33000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:07:  34000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:10:  35000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:14:  36000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:17:  37000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:21:  38000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:24:  39000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:28:  40000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:31:  41000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:35:  42000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:38:  43000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:42:  44000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:45:  45000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:49:  46000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:53:  47000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:33:56:  48000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:00:  49000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:03:  50000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:07:  51000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:10:  52000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:14:  53000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:17:  54000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:21:  55000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:24:  56000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:28:  57000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:31:  58000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:35:  59000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:38:  60000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:42:  61000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:45:  62000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:49:  63000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:52:  64000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:56:  65000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:34:59:  66000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:03:  67000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:06:  68000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:10:  69000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:14:  70000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:17:  71000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:20:  72000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:24:  73000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:27:  74000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:31:  75000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:34:  76000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:37:  77000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:41:  78000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:44:  79000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:48:  80000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:51:  81000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:55:  82000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:35:59:  83000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:02:  84000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:06:  85000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:09:  86000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:13:  87000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:16:  88000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:19:  89000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:23:  90000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:26:  91000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:30:  92000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:33:  93000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:37:  94000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:40:  95000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:44:  96000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:48:  97000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:51:  98000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:55:  99000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:36:58:  100000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:02:  101000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:05:  102000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:08:  103000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:12:  104000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:15:  105000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:19:  106000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:22:  107000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:26:  108000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:29:  109000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:33:  110000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:36:  111000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:40:  112000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:44:  113000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:47:  114000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:51:  115000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:54:  116000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:37:58:  117000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:01:  118000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:04:  119000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:08:  120000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:11:  121000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:15:  122000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:18:  123000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:22:  124000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:25:  125000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:29:  126000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:32:  127000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:36:  128000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:39:  129000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:42:  130000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:45:  131000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:49:  132000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:52:  133000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:56:  134000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:38:59:  135000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:03:  136000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:06:  137000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:10:  138000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:13:  139000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:17:  140000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:20:  141000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:24:  142000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:27:  143000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:31:  144000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:34:  145000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:38:  146000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:41:  147000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  @ Wed, 18 Apr 2018 16:39:44:  148000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:48:  149000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:51:  150000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:55:  151000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:39:58:  152000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:02:  153000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:05:  154000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:09:  155000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:12:  156000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:16:  157000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:19:  158000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:23:  159000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:26:  160000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:30:  161000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:33:  162000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:37:  163000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:40:  164000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:44:  165000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:47:  166000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:50:  167000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:54:  168000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:40:57:  169000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:01:  170000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:04:  171000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:08:  172000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:11:  173000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:14:  174000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:18:  175000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:21:  176000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:25:  177000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:28:  178000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:32:  179000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:35:  180000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:38:  181000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:42:  182000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:45:  183000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:49:  184000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:52:  185000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:56:  186000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:41:59:  187000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:03:  188000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:06:  189000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:09:  190000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:13:  191000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:16:  192000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:20:  193000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:23:  194000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:26:  195000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:30:  196000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:33:  197000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:37:  198000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:40:  199000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:44:  200000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:47:  201000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:50:  202000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:54:  203000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:42:57:  204000000 \n",
      "INFO  @ Wed, 18 Apr 2018 16:43:00:  205000000 \n"
     ]
    }
   ],
   "source": [
    "for index, individual in enumerate(ChIPSeqControl):\n",
    "    outputdirectory = \"test/\" + ChIPSeqTreatment.iloc[index]\n",
    "    name = ChIPSeqTreatment.iloc[index]\n",
    "    immunoprecipitate = \"test/\" + ChIPSeqTreatment.iloc[index] + \".sorted.bam\"\n",
    "    control = \"test/\" + ChIPSeqControl.iloc[index] + \".sorted.bam\"\n",
    "    !macs2 callpeak -c $control -t $immunoprecipitate -n $name --outdir $outputdirectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an in-depth discussion of what MACS2 does: https://github.com/taoliu/MACS/wiki/Advanced:-Call-peaks-using-MACS2-subcommands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedtools\n",
    "\n",
    "In this tutorial, we'll use Bedtools to extract the intersecting regions of the MACS output between the experimental conditions.\n",
    "\n",
    "The Bedtools suite of programs is widely used for genomic interval manipulation or \"genome algebra\". pybedtools wraps and extends Bedtools and offers feature-level manipulations from within Python.\n",
    "\n",
    "First we'll sort the output. The following line uses the `sort` command to sort the MACS output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sort -k 1,1 -k2,2n test/SRR6703661/SRR6703661_peaks.narrowPeak > test/SRR6703661/SRR6703661_peaks.narrowPeak.sorted\n",
    "!sort -k 1,1 -k2,2n test/SRR6703663/SRR6703663_peaks.narrowPeak > test/SRR6703661/SRR6703663_peaks.narrowPeak.sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll use pybedtools to truncate the output files to chromosome so no ends are out of bounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybedtools import BedTool\n",
    "\n",
    "for index, individual in enumerate(ChIPSeqControl):\n",
    "    outputdirectory = \"test/\" + ChIPSeqTreatment.iloc[index]\n",
    "    name = ChIPSeqTreatment.iloc[index]\n",
    "    sort = \"test/\" + name + \"/\" + name + \"_peaks.narrowPeak.sorted\"\n",
    "    truncated = sort + \".truncated\"    \n",
    "    !BedTool($sort).truncate_to_chrom('sacCer3').saveas($truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll find the intersecting regions between the experimental conditions and the control conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bedtools intersect -a test/SRR6703661/SRR6703661.narrowPeak.sorted.truncated -b test/SRR6703661/SRR6703663.narrowPeak.sorted.truncated -u > test/ChIPSeqintersect.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeptools\n",
    "\n",
    "deepTools is a suite of python tools particularly developed for the efficient analysis of high-throughput sequencing data, such as ChIP-seq, RNA-seq or MNase-seq. In this tutorial, we'll use it on ChIP-Seq data first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, individual in enumerate(ChIPSeqControl):\n",
    "    immunoprecipitatename = ChIPSeqTreatment.iloc[index]\n",
    "    immunoprecipitate = \"test/\" + ChIPSeqTreatment.iloc[index] + \".sorted.bam\"\n",
    "    controlname = ChIPSeqControl.iloc[index]\n",
    "    control = \"test/\" + ChIPSeqControl.iloc[index] + \".sorted.bam\"\n",
    "    output = \"test/\" + name + \"/\" + name + \"_coverage.bw\"\n",
    "    !samtools index $immunoprecipitate\n",
    "    !samtools index $control\n",
    "    !bamCoverage --normalizeUsingRPKM -b $immunoprecipitate -o $output\n",
    "    !bamCoverage --normalizeUsingRPKM -b $control -o "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNA-Seq\n",
    "## HTSeq (High-through sequencing)\n",
    "\n",
    "HTSeq is a Python library to facilitate the rapid development of RNA-Seq analysis. HTSeq offers parsers for many common data formats in HTS projects, as well as classes to represent data, such as genomic coordinates, sequences, sequencing reads, alignments, gene model information and variant calls, and provides data structures that allow for querying via genomic coordinates. In this tutorial we will use htseq-count, a tool developed with HTSeq that preprocesses RNA-Seq data for differential expression analysis by counting the overlap of reads with genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "gtf = \"data/Saccharomyces_cerevisiae.R64-1-1.84.gtf\"\n",
    "\n",
    "RNASeqSRARunTableFile='data/RNASeqSRA.tsv'\n",
    "RNASeqSRATable = read_csv(RNASeqSRARunTableFile, delimiter='\\t')\n",
    "RNASeqoutrun = (RNASeqSRATable[\"Run\"]).astype(list)\n",
    "RNASeqoutputSortBam = \"test/\" + RNASeqoutrun + \".sorted.bam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, individual in enumerate(RNASeqoutputSortBam):\n",
    "    input = individual\n",
    "    output = individual + \".genecount.txt\"\n",
    "    !htseq-count -m intersection-nonempty -s no -f bam $input $gtf > $output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## featureCounts\n",
    "\n",
    "featureCounts is a highly efficient general-purpose read summarization program that counts mapped reads for genomic features such as genes, exons, promoter, gene bodies, genomic bins and chromosomal locations. It can be used to count both RNA-seq and genomic DNA-seq reads. It is available in the SourceForge Subread package or the Bioconductor Rsubread package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, individual in enumerate(RNASeqoutputSortBam):\n",
    "    input = individual\n",
    "    !featureCounts -s -a $gtf -o test/featureCounts $input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESeq (Differential Expression Sequencing)\n",
    "\n",
    "Estimate variance-mean dependence in count data from high-throughput sequencing assays and test for differential expression based on a model using the negative binomial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!Rscript scripts/runDeseq.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salmon\n",
    "\n",
    "Salmon is a tool for quantifying the expression of transcripts using RNA-seq data. Salmon uses new algorithms (specifically, coupling the concept of quasi-mapping with a two-phase inference procedure) to provide accurate expression estimates very quickly (i.e. wicked-fast) and while using little memory. Salmon performs its inference using an expressive and realistic model of RNA-seq data that takes into account experimental attributes and biases commonly observed in real RNA-seq data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O data/transcript.fa.gz ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/146/045/GCF_000146045.2_R64/GCF_000146045.2_R64_rna.fna.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Then unzip the transcript.fa.gz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip data/transcript.fa.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finally, create the salmon index and run salmon on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salmon_index = \"test/yeast-salmon.idx\"\n",
    "\n",
    "!salmon index -t data/transcript.fa -i $salmon_index --type quasi -k 31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "RNASeqSRARunTableFile='data/RNASeqSRA.tsv'\n",
    "RNASeqSRATable = read_csv(RNASeqSRARunTableFile, delimiter='\\t')\n",
    "RNASeqoutrun = (RNASeqSRATable[\"Run\"])\n",
    "RNASeqBam = \"test/\" + RNASeqoutrun + \".sorted.bam\"\n",
    "\n",
    "for index, individual in enumerate(RNASeqBam):\n",
    "    !salmon quant --targets $salmon_index -l A --output test -a $individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dupRadar\n",
    "dupRadar provides an easy way to distinguish between artifactual vs natural duplicate reads in RNA-Seq data. Prior to dupRadar only global duplication rates were used and they don't take into account the effect of gene expression levels. dupRadar relates duplication rates and length normalized read counts of every gene to model the dependency of both variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, individual in enumerate(RNASeqoutputSortBam):\n",
    "    !Rscript scripts/dupRadar.R $individual $gtf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
